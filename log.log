[framework] 2020-07-16 22:26:39,987 - org.apache.kafka.clients.producer.ProducerConfig -0    [main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = all
	batch.size = 1000
	bootstrap.servers = [kafka-1:9092]
	buffer.memory = 2048000
	client.dns.lookup = default
	client.id = DemoProducer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.IntegerSerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class cn.neyzoter.module.kafka.AllocationPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[framework] 2020-07-16 22:26:40,065 - org.apache.kafka.common.metrics.Metrics -78   [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bufferpool-wait-time
[framework] 2020-07-16 22:26:40,071 - org.apache.kafka.common.metrics.Metrics -84   [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name buffer-exhausted-records
[framework] 2020-07-16 22:26:40,083 - org.apache.kafka.common.metrics.Metrics -96   [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name errors
[framework] 2020-07-16 22:26:40,091 - org.apache.kafka.common.metrics.Metrics -104  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name produce-throttle-time
[framework] 2020-07-16 22:26:40,102 - org.apache.kafka.common.metrics.Metrics -115  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-closed:
[framework] 2020-07-16 22:26:40,103 - org.apache.kafka.common.metrics.Metrics -116  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-created:
[framework] 2020-07-16 22:26:40,103 - org.apache.kafka.common.metrics.Metrics -116  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name successful-authentication:
[framework] 2020-07-16 22:26:40,103 - org.apache.kafka.common.metrics.Metrics -116  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name successful-reauthentication:
[framework] 2020-07-16 22:26:40,104 - org.apache.kafka.common.metrics.Metrics -117  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name successful-authentication-no-reauth:
[framework] 2020-07-16 22:26:40,104 - org.apache.kafka.common.metrics.Metrics -117  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name failed-authentication:
[framework] 2020-07-16 22:26:40,104 - org.apache.kafka.common.metrics.Metrics -117  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name failed-reauthentication:
[framework] 2020-07-16 22:26:40,104 - org.apache.kafka.common.metrics.Metrics -117  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name reauthentication-latency:
[framework] 2020-07-16 22:26:40,105 - org.apache.kafka.common.metrics.Metrics -118  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent-received:
[framework] 2020-07-16 22:26:40,105 - org.apache.kafka.common.metrics.Metrics -118  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent:
[framework] 2020-07-16 22:26:40,106 - org.apache.kafka.common.metrics.Metrics -119  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-received:
[framework] 2020-07-16 22:26:40,106 - org.apache.kafka.common.metrics.Metrics -119  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name select-time:
[framework] 2020-07-16 22:26:40,107 - org.apache.kafka.common.metrics.Metrics -120  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name io-time:
[framework] 2020-07-16 22:26:40,151 - org.apache.kafka.common.metrics.Metrics -164  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name batch-size
[framework] 2020-07-16 22:26:40,151 - org.apache.kafka.common.metrics.Metrics -164  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name compression-rate
[framework] 2020-07-16 22:26:40,151 - org.apache.kafka.common.metrics.Metrics -164  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name queue-time
[framework] 2020-07-16 22:26:40,151 - org.apache.kafka.common.metrics.Metrics -164  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name request-time
[framework] 2020-07-16 22:26:40,152 - org.apache.kafka.common.metrics.Metrics -165  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-per-request
[framework] 2020-07-16 22:26:40,152 - org.apache.kafka.common.metrics.Metrics -165  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name record-retries
[framework] 2020-07-16 22:26:40,154 - org.apache.kafka.common.metrics.Metrics -167  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name record-size
[framework] 2020-07-16 22:26:40,156 - org.apache.kafka.common.metrics.Metrics -169  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name batch-split-rate
[framework] 2020-07-16 22:26:40,157 - org.apache.kafka.clients.producer.internals.Sender -170  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.producer.internals.Sender  - [Producer clientId=DemoProducer] Starting Kafka producer I/O thread.
[framework] 2020-07-16 22:26:40,160 - org.apache.kafka.common.utils.AppInfoParser -173  [main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version: 2.3.0
[framework] 2020-07-16 22:26:40,160 - org.apache.kafka.common.utils.AppInfoParser -173  [main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId: fc1aaa116b661c8a
[framework] 2020-07-16 22:26:40,160 - org.apache.kafka.common.utils.AppInfoParser -173  [main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka startTimeMs: 1594909600157
[framework] 2020-07-16 22:26:40,162 - org.apache.kafka.clients.producer.KafkaProducer -175  [main] DEBUG org.apache.kafka.clients.producer.KafkaProducer  - [Producer clientId=DemoProducer] Kafka producer started
[framework] 2020-07-16 22:26:40,182 - org.apache.kafka.clients.NetworkClient -195  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Initialize connection to node kafka-1:9092 (id: -1 rack: null) for sending metadata request
[framework] 2020-07-16 22:26:40,183 - org.apache.kafka.clients.NetworkClient -196  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Initiating connection to node kafka-1:9092 (id: -1 rack: null) using address kafka-1/47.110.251.155
[framework] 2020-07-16 22:26:40,212 - kafka.utils.Log4jControllerRegistration$ -225  [main] INFO  kafka.utils.Log4jControllerRegistration$  - Registered kafka:type=kafka.Log4jController MBean
[framework] 2020-07-16 22:26:40,215 - org.apache.kafka.common.metrics.Metrics -228  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.bytes-sent
[framework] 2020-07-16 22:26:40,215 - org.apache.kafka.common.metrics.Metrics -228  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.bytes-received
[framework] 2020-07-16 22:26:40,216 - org.apache.kafka.common.metrics.Metrics -229  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.latency
[framework] 2020-07-16 22:26:40,219 - org.apache.kafka.common.network.Selector -232  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.network.Selector  - [Producer clientId=DemoProducer] Created socket with SO_RCVBUF = 33304, SO_SNDBUF = 131768, SO_TIMEOUT = 0 to node -1
[framework] 2020-07-16 22:26:40,221 - org.apache.kafka.clients.consumer.ConsumerConfig -234  [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [kafka-1:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = consumer1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = DemoConsumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[framework] 2020-07-16 22:26:40,221 - org.apache.kafka.clients.consumer.KafkaConsumer -234  [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - [Consumer clientId=consumer1, groupId=DemoConsumer] Initializing the Kafka consumer
[framework] 2020-07-16 22:26:40,231 - org.apache.kafka.common.metrics.Metrics -244  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-throttle-time
[framework] 2020-07-16 22:26:40,231 - org.apache.kafka.common.metrics.Metrics -244  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-closed:
[framework] 2020-07-16 22:26:40,232 - org.apache.kafka.common.metrics.Metrics -245  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-created:
[framework] 2020-07-16 22:26:40,232 - org.apache.kafka.common.metrics.Metrics -245  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name successful-authentication:
[framework] 2020-07-16 22:26:40,232 - org.apache.kafka.common.metrics.Metrics -245  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name successful-reauthentication:
[framework] 2020-07-16 22:26:40,233 - org.apache.kafka.common.metrics.Metrics -246  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name successful-authentication-no-reauth:
[framework] 2020-07-16 22:26:40,233 - org.apache.kafka.common.metrics.Metrics -246  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name failed-authentication:
[framework] 2020-07-16 22:26:40,233 - org.apache.kafka.common.metrics.Metrics -246  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name failed-reauthentication:
[framework] 2020-07-16 22:26:40,234 - org.apache.kafka.common.metrics.Metrics -247  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name reauthentication-latency:
[framework] 2020-07-16 22:26:40,234 - org.apache.kafka.common.metrics.Metrics -247  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent-received:
[framework] 2020-07-16 22:26:40,234 - org.apache.kafka.common.metrics.Metrics -247  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent:
[framework] 2020-07-16 22:26:40,235 - org.apache.kafka.common.metrics.Metrics -248  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-received:
[framework] 2020-07-16 22:26:40,235 - org.apache.kafka.common.metrics.Metrics -248  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name select-time:
[framework] 2020-07-16 22:26:40,236 - org.apache.kafka.common.metrics.Metrics -249  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name io-time:
[framework] 2020-07-16 22:26:40,249 - org.apache.kafka.common.metrics.Metrics -262  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name heartbeat-latency
[framework] 2020-07-16 22:26:40,249 - org.apache.kafka.common.metrics.Metrics -262  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name join-latency
[framework] 2020-07-16 22:26:40,250 - org.apache.kafka.common.metrics.Metrics -263  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name sync-latency
[framework] 2020-07-16 22:26:40,252 - org.apache.kafka.common.metrics.Metrics -265  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name commit-latency
[framework] 2020-07-16 22:26:40,256 - org.apache.kafka.common.metrics.Metrics -269  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-fetched
[framework] 2020-07-16 22:26:40,256 - org.apache.kafka.common.metrics.Metrics -269  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-fetched
[framework] 2020-07-16 22:26:40,256 - org.apache.kafka.common.metrics.Metrics -269  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-latency
[framework] 2020-07-16 22:26:40,257 - org.apache.kafka.common.metrics.Metrics -270  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-lag
[framework] 2020-07-16 22:26:40,257 - org.apache.kafka.common.metrics.Metrics -270  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-lead
[framework] 2020-07-16 22:26:40,260 - org.apache.kafka.common.utils.AppInfoParser -273  [main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version: 2.3.0
[framework] 2020-07-16 22:26:40,260 - org.apache.kafka.common.utils.AppInfoParser -273  [main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId: fc1aaa116b661c8a
[framework] 2020-07-16 22:26:40,260 - org.apache.kafka.common.utils.AppInfoParser -273  [main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka startTimeMs: 1594909600260
[framework] 2020-07-16 22:26:40,260 - org.apache.kafka.clients.consumer.KafkaConsumer -273  [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - [Consumer clientId=consumer1, groupId=DemoConsumer] Kafka consumer initialized
[framework] 2020-07-16 22:26:40,261 - org.apache.kafka.clients.consumer.KafkaConsumer -274  [main] INFO  org.apache.kafka.clients.consumer.KafkaConsumer  - [Consumer clientId=consumer1, groupId=DemoConsumer] Subscribed to partition(s): topic1-2
[framework] 2020-07-16 22:26:40,272 - cn.neyzoter.module.kafka.KafkaComsumerCli -285  [KafkaConsumerExample_consumer1] INFO  cn.neyzoter.module.kafka.KafkaComsumerCli  - [KafkaConsumerExample_consumer1]: Starting
[framework] 2020-07-16 22:26:40,274 - org.apache.kafka.clients.NetworkClient -287  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.NetworkClient  - [Consumer clientId=consumer1, groupId=DemoConsumer] Initialize connection to node kafka-1:9092 (id: -1 rack: null) for sending metadata request
[framework] 2020-07-16 22:26:40,275 - org.apache.kafka.clients.NetworkClient -288  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.NetworkClient  - [Consumer clientId=consumer1, groupId=DemoConsumer] Initiating connection to node kafka-1:9092 (id: -1 rack: null) using address kafka-1/47.110.251.155
[framework] 2020-07-16 22:26:40,279 - org.apache.kafka.common.metrics.Metrics -292  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.bytes-sent
[framework] 2020-07-16 22:26:40,280 - org.apache.kafka.common.metrics.Metrics -293  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.bytes-received
[framework] 2020-07-16 22:26:40,281 - org.apache.kafka.common.metrics.Metrics -294  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.latency
[framework] 2020-07-16 22:26:40,281 - org.apache.kafka.common.network.Selector -294  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.common.network.Selector  - [Consumer clientId=consumer1, groupId=DemoConsumer] Created socket with SO_RCVBUF = 66608, SO_SNDBUF = 131768, SO_TIMEOUT = 0 to node -1
[framework] 2020-07-16 22:26:40,359 - org.apache.kafka.clients.NetworkClient -372  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Completed connection to node -1. Fetching API versions.
[framework] 2020-07-16 22:26:40,359 - org.apache.kafka.clients.NetworkClient -372  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.NetworkClient  - [Consumer clientId=consumer1, groupId=DemoConsumer] Completed connection to node -1. Fetching API versions.
[framework] 2020-07-16 22:26:40,359 - org.apache.kafka.clients.NetworkClient -372  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Initiating API versions fetch from node -1.
[framework] 2020-07-16 22:26:40,359 - org.apache.kafka.clients.NetworkClient -372  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.NetworkClient  - [Consumer clientId=consumer1, groupId=DemoConsumer] Initiating API versions fetch from node -1.
[framework] 2020-07-16 22:26:40,375 - org.apache.kafka.clients.NetworkClient -388  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.NetworkClient  - [Consumer clientId=consumer1, groupId=DemoConsumer] Recorded API versions for node -1: (Produce(0): 0 to 7 [usable: 7], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 8 [usable: 8], LeaderAndIsr(4): 0 to 2 [usable: 2], StopReplica(5): 0 to 1 [usable: 1], UpdateMetadata(6): 0 to 5 [usable: 5], ControlledShutdown(7): 0 to 2 [usable: 2], OffsetCommit(8): 0 to 7 [usable: 7], OffsetFetch(9): 0 to 5 [usable: 5], FindCoordinator(10): 0 to 2 [usable: 2], JoinGroup(11): 0 to 5 [usable: 5], Heartbeat(12): 0 to 3 [usable: 3], LeaveGroup(13): 0 to 2 [usable: 2], SyncGroup(14): 0 to 3 [usable: 3], DescribeGroups(15): 0 to 3 [usable: 3], ListGroups(16): 0 to 2 [usable: 2], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 2 [usable: 2], CreateTopics(19): 0 to 3 [usable: 3], DeleteTopics(20): 0 to 3 [usable: 3], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 1 [usable: 1], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 1 [usable: 1], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 1 [usable: 1], ElectPreferredLeaders(43): 0 [usable: 0], IncrementalAlterConfigs(44): 0 [usable: 0])
[framework] 2020-07-16 22:26:40,375 - org.apache.kafka.clients.NetworkClient -388  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Recorded API versions for node -1: (Produce(0): 0 to 7 [usable: 7], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 8 [usable: 8], LeaderAndIsr(4): 0 to 2 [usable: 2], StopReplica(5): 0 to 1 [usable: 1], UpdateMetadata(6): 0 to 5 [usable: 5], ControlledShutdown(7): 0 to 2 [usable: 2], OffsetCommit(8): 0 to 7 [usable: 7], OffsetFetch(9): 0 to 5 [usable: 5], FindCoordinator(10): 0 to 2 [usable: 2], JoinGroup(11): 0 to 5 [usable: 5], Heartbeat(12): 0 to 3 [usable: 3], LeaveGroup(13): 0 to 2 [usable: 2], SyncGroup(14): 0 to 3 [usable: 3], DescribeGroups(15): 0 to 3 [usable: 3], ListGroups(16): 0 to 2 [usable: 2], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 2 [usable: 2], CreateTopics(19): 0 to 3 [usable: 3], DeleteTopics(20): 0 to 3 [usable: 3], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 1 [usable: 1], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 1 [usable: 1], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 1 [usable: 1], ElectPreferredLeaders(43): 0 [usable: 0], IncrementalAlterConfigs(44): 0 [usable: 0])
[framework] 2020-07-16 22:26:40,376 - org.apache.kafka.clients.NetworkClient -389  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.NetworkClient  - [Consumer clientId=consumer1, groupId=DemoConsumer] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='topic1')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node kafka-1:9092 (id: -1 rack: null)
[framework] 2020-07-16 22:26:40,376 - org.apache.kafka.clients.NetworkClient -389  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='topic1')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node kafka-1:9092 (id: -1 rack: null)
[framework] 2020-07-16 22:26:40,393 - org.apache.kafka.clients.Metadata -406  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.Metadata  - [Consumer clientId=consumer1, groupId=DemoConsumer] Updating last seen epoch from null to 0 for partition topic1-0
[framework] 2020-07-16 22:26:40,393 - org.apache.kafka.clients.Metadata -406  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from null to 0 for partition topic1-0
[framework] 2020-07-16 22:26:40,394 - org.apache.kafka.clients.Metadata -407  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.Metadata  - [Consumer clientId=consumer1, groupId=DemoConsumer] Updating last seen epoch from null to 0 for partition topic1-2
[framework] 2020-07-16 22:26:40,394 - org.apache.kafka.clients.Metadata -407  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from null to 0 for partition topic1-2
[framework] 2020-07-16 22:26:40,394 - org.apache.kafka.clients.Metadata -407  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.Metadata  - [Consumer clientId=consumer1, groupId=DemoConsumer] Updating last seen epoch from null to 0 for partition topic1-1
[framework] 2020-07-16 22:26:40,394 - org.apache.kafka.clients.Metadata -407  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from null to 0 for partition topic1-1
[framework] 2020-07-16 22:26:40,398 - org.apache.kafka.clients.Metadata -411  [KafkaConsumerExample_consumer1] INFO  org.apache.kafka.clients.Metadata  - [Consumer clientId=consumer1, groupId=DemoConsumer] Cluster ID: 8mkWuJWlQG-6kdWUyVTkZA
[framework] 2020-07-16 22:26:40,398 - org.apache.kafka.clients.Metadata -411  [kafka-producer-network-thread | DemoProducer] INFO  org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Cluster ID: 8mkWuJWlQG-6kdWUyVTkZA
[framework] 2020-07-16 22:26:40,399 - org.apache.kafka.clients.Metadata -412  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updated cluster metadata updateVersion 2 to MetadataCache{cluster=Cluster(id = 8mkWuJWlQG-6kdWUyVTkZA, nodes = [Kafka-1:9092 (id: 1 rack: null)], partitions = [Partition(topic = topic1, partition = 1, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), Partition(topic = topic1, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), Partition(topic = topic1, partition = 2, leader = 1, replicas = [1], isr = [1], offlineReplicas = [])], controller = Kafka-1:9092 (id: 1 rack: null))}
[framework] 2020-07-16 22:26:40,399 - org.apache.kafka.clients.Metadata -412  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.Metadata  - [Consumer clientId=consumer1, groupId=DemoConsumer] Updated cluster metadata updateVersion 2 to MetadataCache{cluster=Cluster(id = 8mkWuJWlQG-6kdWUyVTkZA, nodes = [Kafka-1:9092 (id: 1 rack: null)], partitions = [Partition(topic = topic1, partition = 1, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), Partition(topic = topic1, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), Partition(topic = topic1, partition = 2, leader = 1, replicas = [1], isr = [1], offlineReplicas = [])], controller = Kafka-1:9092 (id: 1 rack: null))}
[framework] 2020-07-16 22:26:40,410 - org.apache.kafka.clients.consumer.internals.AbstractCoordinator -423  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - [Consumer clientId=consumer1, groupId=DemoConsumer] Sending FindCoordinator request to broker Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:40,412 - org.apache.kafka.clients.NetworkClient -425  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.NetworkClient  - [Consumer clientId=consumer1, groupId=DemoConsumer] Initiating connection to node Kafka-1:9092 (id: 1 rack: null) using address kafka-1/47.110.251.155
[framework] 2020-07-16 22:26:40,415 - org.apache.kafka.common.metrics.Metrics -428  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-1.bytes-sent
[framework] 2020-07-16 22:26:40,416 - org.apache.kafka.common.metrics.Metrics -429  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-1.bytes-received
[framework] 2020-07-16 22:26:40,417 - org.apache.kafka.common.metrics.Metrics -430  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-1.latency
[framework] 2020-07-16 22:26:40,417 - org.apache.kafka.common.network.Selector -430  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.common.network.Selector  - [Consumer clientId=consumer1, groupId=DemoConsumer] Created socket with SO_RCVBUF = 66608, SO_SNDBUF = 131768, SO_TIMEOUT = 0 to node 1
[framework] 2020-07-16 22:26:40,417 - org.apache.kafka.clients.NetworkClient -430  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.NetworkClient  - [Consumer clientId=consumer1, groupId=DemoConsumer] Completed connection to node 1. Fetching API versions.
[framework] 2020-07-16 22:26:40,417 - org.apache.kafka.clients.NetworkClient -430  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.NetworkClient  - [Consumer clientId=consumer1, groupId=DemoConsumer] Initiating API versions fetch from node 1.
[framework] 2020-07-16 22:26:40,422 - org.apache.kafka.clients.NetworkClient -435  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.NetworkClient  - [Consumer clientId=consumer1, groupId=DemoConsumer] Recorded API versions for node 1: (Produce(0): 0 to 7 [usable: 7], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 8 [usable: 8], LeaderAndIsr(4): 0 to 2 [usable: 2], StopReplica(5): 0 to 1 [usable: 1], UpdateMetadata(6): 0 to 5 [usable: 5], ControlledShutdown(7): 0 to 2 [usable: 2], OffsetCommit(8): 0 to 7 [usable: 7], OffsetFetch(9): 0 to 5 [usable: 5], FindCoordinator(10): 0 to 2 [usable: 2], JoinGroup(11): 0 to 5 [usable: 5], Heartbeat(12): 0 to 3 [usable: 3], LeaveGroup(13): 0 to 2 [usable: 2], SyncGroup(14): 0 to 3 [usable: 3], DescribeGroups(15): 0 to 3 [usable: 3], ListGroups(16): 0 to 2 [usable: 2], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 2 [usable: 2], CreateTopics(19): 0 to 3 [usable: 3], DeleteTopics(20): 0 to 3 [usable: 3], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 1 [usable: 1], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 1 [usable: 1], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 1 [usable: 1], ElectPreferredLeaders(43): 0 [usable: 0], IncrementalAlterConfigs(44): 0 [usable: 0])
[framework] 2020-07-16 22:26:40,431 - org.apache.kafka.clients.consumer.internals.AbstractCoordinator -444  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - [Consumer clientId=consumer1, groupId=DemoConsumer] Received FindCoordinator response ClientResponse(receivedTimeMs=1594909600430, latencyMs=20, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=2, clientId=consumer1, correlationId=2), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='NONE', nodeId=1, host='Kafka-1', port=9092))
[framework] 2020-07-16 22:26:40,431 - org.apache.kafka.clients.consumer.internals.AbstractCoordinator -444  [KafkaConsumerExample_consumer1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - [Consumer clientId=consumer1, groupId=DemoConsumer] Discovered group coordinator Kafka-1:9092 (id: 2147483646 rack: null)
[framework] 2020-07-16 22:26:40,432 - org.apache.kafka.clients.NetworkClient -445  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.NetworkClient  - [Consumer clientId=consumer1, groupId=DemoConsumer] Initiating connection to node Kafka-1:9092 (id: 2147483646 rack: null) using address kafka-1/47.110.251.155
[framework] 2020-07-16 22:26:40,432 - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -445  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer1, groupId=DemoConsumer] Fetching committed offsets for partitions: [topic1-2]
[framework] 2020-07-16 22:26:40,437 - org.apache.kafka.common.metrics.Metrics -450  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2147483646.bytes-sent
[framework] 2020-07-16 22:26:40,437 - org.apache.kafka.common.metrics.Metrics -450  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2147483646.bytes-received
[framework] 2020-07-16 22:26:40,438 - org.apache.kafka.common.metrics.Metrics -451  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2147483646.latency
[framework] 2020-07-16 22:26:40,438 - org.apache.kafka.common.network.Selector -451  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.common.network.Selector  - [Consumer clientId=consumer1, groupId=DemoConsumer] Created socket with SO_RCVBUF = 66608, SO_SNDBUF = 131768, SO_TIMEOUT = 0 to node 2147483646
[framework] 2020-07-16 22:26:40,438 - org.apache.kafka.clients.NetworkClient -451  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.NetworkClient  - [Consumer clientId=consumer1, groupId=DemoConsumer] Completed connection to node 2147483646. Fetching API versions.
[framework] 2020-07-16 22:26:40,438 - org.apache.kafka.clients.NetworkClient -451  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.NetworkClient  - [Consumer clientId=consumer1, groupId=DemoConsumer] Initiating API versions fetch from node 2147483646.
[framework] 2020-07-16 22:26:40,444 - org.apache.kafka.clients.NetworkClient -457  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.NetworkClient  - [Consumer clientId=consumer1, groupId=DemoConsumer] Recorded API versions for node 2147483646: (Produce(0): 0 to 7 [usable: 7], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 8 [usable: 8], LeaderAndIsr(4): 0 to 2 [usable: 2], StopReplica(5): 0 to 1 [usable: 1], UpdateMetadata(6): 0 to 5 [usable: 5], ControlledShutdown(7): 0 to 2 [usable: 2], OffsetCommit(8): 0 to 7 [usable: 7], OffsetFetch(9): 0 to 5 [usable: 5], FindCoordinator(10): 0 to 2 [usable: 2], JoinGroup(11): 0 to 5 [usable: 5], Heartbeat(12): 0 to 3 [usable: 3], LeaveGroup(13): 0 to 2 [usable: 2], SyncGroup(14): 0 to 3 [usable: 3], DescribeGroups(15): 0 to 3 [usable: 3], ListGroups(16): 0 to 2 [usable: 2], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 2 [usable: 2], CreateTopics(19): 0 to 3 [usable: 3], DeleteTopics(20): 0 to 3 [usable: 3], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 1 [usable: 1], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 1 [usable: 1], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 1 [usable: 1], ElectPreferredLeaders(43): 0 [usable: 0], IncrementalAlterConfigs(44): 0 [usable: 0])
[framework] 2020-07-16 22:26:40,451 - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -464  [KafkaConsumerExample_consumer1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer1, groupId=DemoConsumer] Setting offset for partition topic1-2 to the committed offset FetchPosition{offset=74556, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Kafka-1:9092 (id: 1 rack: null), epoch=0}}
[framework] 2020-07-16 22:26:40,452 - org.apache.kafka.clients.Metadata -465  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.Metadata  - [Consumer clientId=consumer1, groupId=DemoConsumer] Not replacing existing epoch 0 with new epoch 0
[framework] 2020-07-16 22:26:40,469 - org.apache.kafka.clients.consumer.internals.OffsetsForLeaderEpochClient -482  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.OffsetsForLeaderEpochClient  - [Consumer clientId=consumer1, groupId=DemoConsumer] Handling OffsetsForLeaderEpoch response for topic1-2. Got offset 74556 for epoch 0
[framework] 2020-07-16 22:26:40,473 - org.apache.kafka.clients.consumer.internals.Fetcher -486  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Added READ_UNCOMMITTED fetch request for partition topic1-2 at position FetchPosition{offset=74556, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Kafka-1:9092 (id: 1 rack: null), epoch=0}} to node Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:40,473 - org.apache.kafka.clients.FetchSessionHandler -486  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Built full fetch (sessionId=INVALID, epoch=INITIAL) for node 1 with 1 partition(s).
[framework] 2020-07-16 22:26:40,474 - org.apache.kafka.clients.consumer.internals.Fetcher -487  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Sending READ_UNCOMMITTED FullFetchRequest(topic1-2) to broker Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:40,912 - org.apache.kafka.clients.NetworkClient -925  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Initiating connection to node Kafka-1:9092 (id: 1 rack: null) using address kafka-1/47.110.251.155
[framework] 2020-07-16 22:26:40,917 - org.apache.kafka.common.metrics.Metrics -930  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-1.bytes-sent
[framework] 2020-07-16 22:26:40,918 - org.apache.kafka.common.metrics.Metrics -931  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-1.bytes-received
[framework] 2020-07-16 22:26:40,919 - org.apache.kafka.common.metrics.Metrics -932  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-1.latency
[framework] 2020-07-16 22:26:40,919 - org.apache.kafka.common.network.Selector -932  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.network.Selector  - [Producer clientId=DemoProducer] Created socket with SO_RCVBUF = 33304, SO_SNDBUF = 131768, SO_TIMEOUT = 0 to node 1
[framework] 2020-07-16 22:26:40,919 - org.apache.kafka.clients.NetworkClient -932  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Completed connection to node 1. Fetching API versions.
[framework] 2020-07-16 22:26:40,919 - org.apache.kafka.clients.NetworkClient -932  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Initiating API versions fetch from node 1.
[framework] 2020-07-16 22:26:40,924 - org.apache.kafka.clients.NetworkClient -937  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Recorded API versions for node 1: (Produce(0): 0 to 7 [usable: 7], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 8 [usable: 8], LeaderAndIsr(4): 0 to 2 [usable: 2], StopReplica(5): 0 to 1 [usable: 1], UpdateMetadata(6): 0 to 5 [usable: 5], ControlledShutdown(7): 0 to 2 [usable: 2], OffsetCommit(8): 0 to 7 [usable: 7], OffsetFetch(9): 0 to 5 [usable: 5], FindCoordinator(10): 0 to 2 [usable: 2], JoinGroup(11): 0 to 5 [usable: 5], Heartbeat(12): 0 to 3 [usable: 3], LeaveGroup(13): 0 to 2 [usable: 2], SyncGroup(14): 0 to 3 [usable: 3], DescribeGroups(15): 0 to 3 [usable: 3], ListGroups(16): 0 to 2 [usable: 2], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 2 [usable: 2], CreateTopics(19): 0 to 3 [usable: 3], DeleteTopics(20): 0 to 3 [usable: 3], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 1 [usable: 1], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 1 [usable: 1], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 1 [usable: 1], ElectPreferredLeaders(43): 0 [usable: 0], IncrementalAlterConfigs(44): 0 [usable: 0])
[framework] 2020-07-16 22:26:40,930 - org.apache.kafka.common.metrics.Metrics -943  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.topic1.records-per-batch
[framework] 2020-07-16 22:26:40,930 - org.apache.kafka.common.metrics.Metrics -943  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.topic1.bytes
[framework] 2020-07-16 22:26:40,930 - org.apache.kafka.common.metrics.Metrics -943  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.topic1.compression-rate
[framework] 2020-07-16 22:26:40,931 - org.apache.kafka.common.metrics.Metrics -944  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.topic1.record-retries
[framework] 2020-07-16 22:26:40,931 - org.apache.kafka.common.metrics.Metrics -944  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.topic1.record-errors
[framework] 2020-07-16 22:26:40,956 - org.apache.kafka.clients.FetchSessionHandler -969  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Node 1 sent a full fetch response that created a new incremental fetch session 460818967 with 1 response partition(s)
[framework] 2020-07-16 22:26:40,958 - org.apache.kafka.clients.consumer.internals.Fetcher -971  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Fetch READ_UNCOMMITTED at offset 74556 for partition topic1-2 returned fetch data (error=NONE, highWaterMark=74571, lastStableOffset = 74571, logStartOffset = 74396, preferredReadReplica = absent, abortedTransactions = null, recordsSizeInBytes=386)
[framework] 2020-07-16 22:26:40,961 - org.apache.kafka.common.metrics.Metrics -974  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.topic1.bytes-fetched
[framework] 2020-07-16 22:26:40,962 - org.apache.kafka.common.metrics.Metrics -975  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.topic1.records-fetched
[framework] 2020-07-16 22:26:40,962 - org.apache.kafka.common.metrics.Metrics -975  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic1-2.records-lag
[framework] 2020-07-16 22:26:40,963 - org.apache.kafka.common.metrics.Metrics -976  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic1-2.records-lead
[framework] 2020-07-16 22:26:40,964 - org.apache.kafka.clients.consumer.internals.Fetcher -977  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Added READ_UNCOMMITTED fetch request for partition topic1-2 at position FetchPosition{offset=74571, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Kafka-1:9092 (id: 1 rack: null), epoch=0}} to node Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:40,964 - org.apache.kafka.clients.FetchSessionHandler -977  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Built incremental fetch (sessionId=460818967, epoch=1) for node 1. Added 0 partition(s), altered 1 partition(s), removed 0 partition(s) out of 1 partition(s)
[framework] 2020-07-16 22:26:40,964 - org.apache.kafka.clients.consumer.internals.Fetcher -977  [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(topic1-2), toForget=(), implied=()) to broker Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:41,256 - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -1269 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer1, groupId=DemoConsumer] Sending asynchronous auto-commit of offsets {topic1-2=OffsetAndMetadata{offset=74571, leaderEpoch=0, metadata=''}}
[framework] 2020-07-16 22:26:41,281 - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -1294 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer1, groupId=DemoConsumer] Committed offset 74571 for partition topic1-2
[framework] 2020-07-16 22:26:41,282 - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -1295 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer1, groupId=DemoConsumer] Completed asynchronous auto-commit of offsets {topic1-2=OffsetAndMetadata{offset=74571, leaderEpoch=0, metadata=''}}
[framework] 2020-07-16 22:26:41,442 - org.apache.kafka.clients.FetchSessionHandler -1455 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Node 1 sent an incremental fetch response for session 460818967 with 1 response partition(s)
[framework] 2020-07-16 22:26:41,442 - org.apache.kafka.clients.consumer.internals.Fetcher -1455 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Fetch READ_UNCOMMITTED at offset 74571 for partition topic1-2 returned fetch data (error=NONE, highWaterMark=74585, lastStableOffset = 74585, logStartOffset = 74396, preferredReadReplica = absent, abortedTransactions = null, recordsSizeInBytes=367)
[framework] 2020-07-16 22:26:41,443 - org.apache.kafka.clients.consumer.internals.Fetcher -1456 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Added READ_UNCOMMITTED fetch request for partition topic1-2 at position FetchPosition{offset=74585, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Kafka-1:9092 (id: 1 rack: null), epoch=0}} to node Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:41,443 - org.apache.kafka.clients.FetchSessionHandler -1456 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Built incremental fetch (sessionId=460818967, epoch=2) for node 1. Added 0 partition(s), altered 1 partition(s), removed 0 partition(s) out of 1 partition(s)
[framework] 2020-07-16 22:26:41,443 - org.apache.kafka.clients.consumer.internals.Fetcher -1456 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(topic1-2), toForget=(), implied=()) to broker Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:41,955 - org.apache.kafka.clients.FetchSessionHandler -1968 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Node 1 sent an incremental fetch response for session 460818967 with 0 response partition(s), 1 implied partition(s)
[framework] 2020-07-16 22:26:41,955 - org.apache.kafka.clients.consumer.internals.Fetcher -1968 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Added READ_UNCOMMITTED fetch request for partition topic1-2 at position FetchPosition{offset=74585, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Kafka-1:9092 (id: 1 rack: null), epoch=0}} to node Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:41,955 - org.apache.kafka.clients.FetchSessionHandler -1968 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Built incremental fetch (sessionId=460818967, epoch=3) for node 1. Added 0 partition(s), altered 0 partition(s), removed 0 partition(s) out of 1 partition(s)
[framework] 2020-07-16 22:26:41,955 - org.apache.kafka.clients.consumer.internals.Fetcher -1968 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(), toForget=(), implied=(topic1-2)) to broker Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:41,961 - org.apache.kafka.clients.FetchSessionHandler -1974 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Node 1 sent an incremental fetch response for session 460818967 with 1 response partition(s)
[framework] 2020-07-16 22:26:41,961 - org.apache.kafka.clients.consumer.internals.Fetcher -1974 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Fetch READ_UNCOMMITTED at offset 74585 for partition topic1-2 returned fetch data (error=NONE, highWaterMark=74600, lastStableOffset = 74600, logStartOffset = 74396, preferredReadReplica = absent, abortedTransactions = null, recordsSizeInBytes=400)
[framework] 2020-07-16 22:26:41,961 - org.apache.kafka.clients.consumer.internals.Fetcher -1974 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Added READ_UNCOMMITTED fetch request for partition topic1-2 at position FetchPosition{offset=74600, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Kafka-1:9092 (id: 1 rack: null), epoch=0}} to node Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:41,962 - org.apache.kafka.clients.FetchSessionHandler -1975 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Built incremental fetch (sessionId=460818967, epoch=4) for node 1. Added 0 partition(s), altered 1 partition(s), removed 0 partition(s) out of 1 partition(s)
[framework] 2020-07-16 22:26:41,962 - org.apache.kafka.clients.consumer.internals.Fetcher -1975 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(topic1-2), toForget=(), implied=()) to broker Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:42,260 - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -2273 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer1, groupId=DemoConsumer] Sending asynchronous auto-commit of offsets {topic1-2=OffsetAndMetadata{offset=74600, leaderEpoch=0, metadata=''}}
[framework] 2020-07-16 22:26:42,273 - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -2286 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer1, groupId=DemoConsumer] Committed offset 74600 for partition topic1-2
[framework] 2020-07-16 22:26:42,273 - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -2286 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer1, groupId=DemoConsumer] Completed asynchronous auto-commit of offsets {topic1-2=OffsetAndMetadata{offset=74600, leaderEpoch=0, metadata=''}}
[framework] 2020-07-16 22:26:42,468 - org.apache.kafka.clients.FetchSessionHandler -2481 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Node 1 sent an incremental fetch response for session 460818967 with 1 response partition(s)
[framework] 2020-07-16 22:26:42,469 - org.apache.kafka.clients.consumer.internals.Fetcher -2482 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Fetch READ_UNCOMMITTED at offset 74600 for partition topic1-2 returned fetch data (error=NONE, highWaterMark=74614, lastStableOffset = 74614, logStartOffset = 74396, preferredReadReplica = absent, abortedTransactions = null, recordsSizeInBytes=381)
[framework] 2020-07-16 22:26:42,470 - org.apache.kafka.clients.consumer.internals.Fetcher -2483 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Added READ_UNCOMMITTED fetch request for partition topic1-2 at position FetchPosition{offset=74614, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Kafka-1:9092 (id: 1 rack: null), epoch=0}} to node Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:42,471 - org.apache.kafka.clients.FetchSessionHandler -2484 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Built incremental fetch (sessionId=460818967, epoch=5) for node 1. Added 0 partition(s), altered 1 partition(s), removed 0 partition(s) out of 1 partition(s)
[framework] 2020-07-16 22:26:42,472 - org.apache.kafka.clients.consumer.internals.Fetcher -2485 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(topic1-2), toForget=(), implied=()) to broker Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:42,979 - org.apache.kafka.clients.FetchSessionHandler -2992 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Node 1 sent an incremental fetch response for session 460818967 with 0 response partition(s), 1 implied partition(s)
[framework] 2020-07-16 22:26:42,980 - org.apache.kafka.clients.consumer.internals.Fetcher -2993 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Added READ_UNCOMMITTED fetch request for partition topic1-2 at position FetchPosition{offset=74614, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Kafka-1:9092 (id: 1 rack: null), epoch=0}} to node Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:42,980 - org.apache.kafka.clients.FetchSessionHandler -2993 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Built incremental fetch (sessionId=460818967, epoch=6) for node 1. Added 0 partition(s), altered 0 partition(s), removed 0 partition(s) out of 1 partition(s)
[framework] 2020-07-16 22:26:42,980 - org.apache.kafka.clients.consumer.internals.Fetcher -2993 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(), toForget=(), implied=(topic1-2)) to broker Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:42,986 - org.apache.kafka.clients.FetchSessionHandler -2999 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Node 1 sent an incremental fetch response for session 460818967 with 1 response partition(s)
[framework] 2020-07-16 22:26:42,986 - org.apache.kafka.clients.consumer.internals.Fetcher -2999 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Fetch READ_UNCOMMITTED at offset 74614 for partition topic1-2 returned fetch data (error=NONE, highWaterMark=74629, lastStableOffset = 74629, logStartOffset = 74396, preferredReadReplica = absent, abortedTransactions = null, recordsSizeInBytes=404)
[framework] 2020-07-16 22:26:42,987 - org.apache.kafka.clients.consumer.internals.Fetcher -3000 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Added READ_UNCOMMITTED fetch request for partition topic1-2 at position FetchPosition{offset=74629, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Kafka-1:9092 (id: 1 rack: null), epoch=0}} to node Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:42,987 - org.apache.kafka.clients.FetchSessionHandler -3000 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Built incremental fetch (sessionId=460818967, epoch=7) for node 1. Added 0 partition(s), altered 1 partition(s), removed 0 partition(s) out of 1 partition(s)
[framework] 2020-07-16 22:26:42,987 - org.apache.kafka.clients.consumer.internals.Fetcher -3000 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(topic1-2), toForget=(), implied=()) to broker Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:43,265 - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -3278 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer1, groupId=DemoConsumer] Sending asynchronous auto-commit of offsets {topic1-2=OffsetAndMetadata{offset=74629, leaderEpoch=0, metadata=''}}
[framework] 2020-07-16 22:26:43,279 - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -3292 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer1, groupId=DemoConsumer] Committed offset 74629 for partition topic1-2
[framework] 2020-07-16 22:26:43,279 - org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -3292 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer1, groupId=DemoConsumer] Completed asynchronous auto-commit of offsets {topic1-2=OffsetAndMetadata{offset=74629, leaderEpoch=0, metadata=''}}
[framework] 2020-07-16 22:26:43,492 - org.apache.kafka.clients.FetchSessionHandler -3505 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Node 1 sent an incremental fetch response for session 460818967 with 1 response partition(s)
[framework] 2020-07-16 22:26:43,492 - org.apache.kafka.clients.consumer.internals.Fetcher -3505 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Fetch READ_UNCOMMITTED at offset 74629 for partition topic1-2 returned fetch data (error=NONE, highWaterMark=74643, lastStableOffset = 74643, logStartOffset = 74396, preferredReadReplica = absent, abortedTransactions = null, recordsSizeInBytes=381)
[framework] 2020-07-16 22:26:43,493 - org.apache.kafka.clients.consumer.internals.Fetcher -3506 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Added READ_UNCOMMITTED fetch request for partition topic1-2 at position FetchPosition{offset=74643, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Kafka-1:9092 (id: 1 rack: null), epoch=0}} to node Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:43,494 - org.apache.kafka.clients.FetchSessionHandler -3507 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Built incremental fetch (sessionId=460818967, epoch=8) for node 1. Added 0 partition(s), altered 1 partition(s), removed 0 partition(s) out of 1 partition(s)
[framework] 2020-07-16 22:26:43,494 - org.apache.kafka.clients.consumer.internals.Fetcher -3507 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(topic1-2), toForget=(), implied=()) to broker Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:43,999 - org.apache.kafka.clients.FetchSessionHandler -4012 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Node 1 sent an incremental fetch response for session 460818967 with 1 response partition(s)
[framework] 2020-07-16 22:26:43,999 - org.apache.kafka.clients.consumer.internals.Fetcher -4012 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Fetch READ_UNCOMMITTED at offset 74643 for partition topic1-2 returned fetch data (error=NONE, highWaterMark=74658, lastStableOffset = 74658, logStartOffset = 74396, preferredReadReplica = absent, abortedTransactions = null, recordsSizeInBytes=404)
[framework] 2020-07-16 22:26:44,000 - org.apache.kafka.clients.consumer.internals.Fetcher -4013 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Added READ_UNCOMMITTED fetch request for partition topic1-2 at position FetchPosition{offset=74658, offsetEpoch=Optional[0], currentLeader=LeaderAndEpoch{leader=Kafka-1:9092 (id: 1 rack: null), epoch=0}} to node Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-07-16 22:26:44,001 - org.apache.kafka.clients.FetchSessionHandler -4014 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.FetchSessionHandler  - [Consumer clientId=consumer1, groupId=DemoConsumer] Built incremental fetch (sessionId=460818967, epoch=9) for node 1. Added 0 partition(s), altered 1 partition(s), removed 0 partition(s) out of 1 partition(s)
[framework] 2020-07-16 22:26:44,001 - org.apache.kafka.clients.consumer.internals.Fetcher -4014 [KafkaConsumerExample_consumer1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - [Consumer clientId=consumer1, groupId=DemoConsumer] Sending READ_UNCOMMITTED IncrementalFetchRequest(toSend=(topic1-2), toForget=(), implied=()) to broker Kafka-1:9092 (id: 1 rack: null)
