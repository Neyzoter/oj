[framework] 2020-06-21 08:48:01,667 - org.apache.kafka.clients.producer.ProducerConfig -0    [main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka-1:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = DemoProducer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.IntegerSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class cn.neyzoter.module.kafka.AllocationPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[framework] 2020-06-21 08:48:01,748 - org.apache.kafka.common.metrics.Metrics -81   [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bufferpool-wait-time
[framework] 2020-06-21 08:48:01,753 - org.apache.kafka.common.metrics.Metrics -86   [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name buffer-exhausted-records
[framework] 2020-06-21 08:48:01,759 - org.apache.kafka.common.metrics.Metrics -92   [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name errors
[framework] 2020-06-21 08:48:01,768 - org.apache.kafka.common.metrics.Metrics -101  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name produce-throttle-time
[framework] 2020-06-21 08:48:01,778 - org.apache.kafka.common.metrics.Metrics -111  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-closed:
[framework] 2020-06-21 08:48:01,778 - org.apache.kafka.common.metrics.Metrics -111  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-created:
[framework] 2020-06-21 08:48:01,779 - org.apache.kafka.common.metrics.Metrics -112  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name successful-authentication:
[framework] 2020-06-21 08:48:01,779 - org.apache.kafka.common.metrics.Metrics -112  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name successful-reauthentication:
[framework] 2020-06-21 08:48:01,780 - org.apache.kafka.common.metrics.Metrics -113  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name successful-authentication-no-reauth:
[framework] 2020-06-21 08:48:01,780 - org.apache.kafka.common.metrics.Metrics -113  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name failed-authentication:
[framework] 2020-06-21 08:48:01,780 - org.apache.kafka.common.metrics.Metrics -113  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name failed-reauthentication:
[framework] 2020-06-21 08:48:01,780 - org.apache.kafka.common.metrics.Metrics -113  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name reauthentication-latency:
[framework] 2020-06-21 08:48:01,781 - org.apache.kafka.common.metrics.Metrics -114  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent-received:
[framework] 2020-06-21 08:48:01,781 - org.apache.kafka.common.metrics.Metrics -114  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent:
[framework] 2020-06-21 08:48:01,782 - org.apache.kafka.common.metrics.Metrics -115  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-received:
[framework] 2020-06-21 08:48:01,782 - org.apache.kafka.common.metrics.Metrics -115  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name select-time:
[framework] 2020-06-21 08:48:01,783 - org.apache.kafka.common.metrics.Metrics -116  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name io-time:
[framework] 2020-06-21 08:48:01,836 - org.apache.kafka.common.metrics.Metrics -169  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name batch-size
[framework] 2020-06-21 08:48:01,837 - org.apache.kafka.common.metrics.Metrics -170  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name compression-rate
[framework] 2020-06-21 08:48:01,837 - org.apache.kafka.common.metrics.Metrics -170  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name queue-time
[framework] 2020-06-21 08:48:01,837 - org.apache.kafka.common.metrics.Metrics -170  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name request-time
[framework] 2020-06-21 08:48:01,838 - org.apache.kafka.common.metrics.Metrics -171  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-per-request
[framework] 2020-06-21 08:48:01,838 - org.apache.kafka.common.metrics.Metrics -171  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name record-retries
[framework] 2020-06-21 08:48:01,838 - org.apache.kafka.common.metrics.Metrics -171  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name record-size
[framework] 2020-06-21 08:48:01,842 - org.apache.kafka.common.metrics.Metrics -175  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name batch-split-rate
[framework] 2020-06-21 08:48:01,844 - org.apache.kafka.clients.producer.internals.Sender -177  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.producer.internals.Sender  - [Producer clientId=DemoProducer] Starting Kafka producer I/O thread.
[framework] 2020-06-21 08:48:01,846 - org.apache.kafka.common.utils.AppInfoParser -179  [main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version: 2.3.0
[framework] 2020-06-21 08:48:01,846 - org.apache.kafka.common.utils.AppInfoParser -179  [main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId: fc1aaa116b661c8a
[framework] 2020-06-21 08:48:01,846 - org.apache.kafka.common.utils.AppInfoParser -179  [main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka startTimeMs: 1592700481844
[framework] 2020-06-21 08:48:01,848 - org.apache.kafka.clients.producer.KafkaProducer -181  [main] DEBUG org.apache.kafka.clients.producer.KafkaProducer  - [Producer clientId=DemoProducer] Kafka producer started
[framework] 2020-06-21 08:48:01,863 - org.apache.kafka.clients.NetworkClient -196  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Initialize connection to node kafka-1:9092 (id: -1 rack: null) for sending metadata request
[framework] 2020-06-21 08:48:01,865 - org.apache.kafka.clients.NetworkClient -198  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Initiating connection to node kafka-1:9092 (id: -1 rack: null) using address kafka-1/47.110.251.155
[framework] 2020-06-21 08:48:01,876 - org.apache.kafka.common.metrics.Metrics -209  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.bytes-sent
[framework] 2020-06-21 08:48:01,877 - org.apache.kafka.common.metrics.Metrics -210  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.bytes-received
[framework] 2020-06-21 08:48:01,878 - org.apache.kafka.common.metrics.Metrics -211  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.latency
[framework] 2020-06-21 08:48:01,878 - org.apache.kafka.common.network.Selector -211  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.network.Selector  - [Producer clientId=DemoProducer] Created socket with SO_RCVBUF = 32768, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node -1
[framework] 2020-06-21 08:48:01,898 - kafka.utils.Log4jControllerRegistration$ -231  [main] INFO  kafka.utils.Log4jControllerRegistration$  - Registered kafka:type=kafka.Log4jController MBean
[framework] 2020-06-21 08:48:01,909 - org.apache.kafka.clients.consumer.ConsumerConfig -242  [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [kafka-1:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = DemoConsumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[framework] 2020-06-21 08:48:01,909 - org.apache.kafka.clients.consumer.KafkaConsumer -242  [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - [Consumer clientId=consumer, groupId=DemoConsumer] Initializing the Kafka consumer
[framework] 2020-06-21 08:48:01,924 - org.apache.kafka.common.metrics.Metrics -257  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-throttle-time
[framework] 2020-06-21 08:48:01,924 - org.apache.kafka.common.metrics.Metrics -257  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-closed:
[framework] 2020-06-21 08:48:01,925 - org.apache.kafka.common.metrics.Metrics -258  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-created:
[framework] 2020-06-21 08:48:01,925 - org.apache.kafka.common.metrics.Metrics -258  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name successful-authentication:
[framework] 2020-06-21 08:48:01,925 - org.apache.kafka.common.metrics.Metrics -258  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name successful-reauthentication:
[framework] 2020-06-21 08:48:01,926 - org.apache.kafka.common.metrics.Metrics -259  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name successful-authentication-no-reauth:
[framework] 2020-06-21 08:48:01,926 - org.apache.kafka.common.metrics.Metrics -259  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name failed-authentication:
[framework] 2020-06-21 08:48:01,927 - org.apache.kafka.common.metrics.Metrics -260  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name failed-reauthentication:
[framework] 2020-06-21 08:48:01,927 - org.apache.kafka.common.metrics.Metrics -260  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name reauthentication-latency:
[framework] 2020-06-21 08:48:01,927 - org.apache.kafka.common.metrics.Metrics -260  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent-received:
[framework] 2020-06-21 08:48:01,928 - org.apache.kafka.common.metrics.Metrics -261  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent:
[framework] 2020-06-21 08:48:01,929 - org.apache.kafka.common.metrics.Metrics -262  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-received:
[framework] 2020-06-21 08:48:01,930 - org.apache.kafka.common.metrics.Metrics -263  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name select-time:
[framework] 2020-06-21 08:48:01,930 - org.apache.kafka.common.metrics.Metrics -263  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name io-time:
[framework] 2020-06-21 08:48:01,947 - org.apache.kafka.common.metrics.Metrics -280  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name heartbeat-latency
[framework] 2020-06-21 08:48:01,948 - org.apache.kafka.common.metrics.Metrics -281  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name join-latency
[framework] 2020-06-21 08:48:01,949 - org.apache.kafka.common.metrics.Metrics -282  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name sync-latency
[framework] 2020-06-21 08:48:01,951 - org.apache.kafka.common.metrics.Metrics -284  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name commit-latency
[framework] 2020-06-21 08:48:01,954 - org.apache.kafka.common.metrics.Metrics -287  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-fetched
[framework] 2020-06-21 08:48:01,955 - org.apache.kafka.common.metrics.Metrics -288  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-fetched
[framework] 2020-06-21 08:48:01,955 - org.apache.kafka.common.metrics.Metrics -288  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-latency
[framework] 2020-06-21 08:48:01,956 - org.apache.kafka.common.metrics.Metrics -289  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-lag
[framework] 2020-06-21 08:48:01,956 - org.apache.kafka.common.metrics.Metrics -289  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-lead
[framework] 2020-06-21 08:48:01,958 - org.apache.kafka.common.utils.AppInfoParser -291  [main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version: 2.3.0
[framework] 2020-06-21 08:48:01,958 - org.apache.kafka.common.utils.AppInfoParser -291  [main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId: fc1aaa116b661c8a
[framework] 2020-06-21 08:48:01,959 - org.apache.kafka.common.utils.AppInfoParser -292  [main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka startTimeMs: 1592700481958
[framework] 2020-06-21 08:48:01,959 - org.apache.kafka.clients.consumer.KafkaConsumer -292  [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - [Consumer clientId=consumer, groupId=DemoConsumer] Kafka consumer initialized
[framework] 2020-06-21 08:48:01,960 - org.apache.kafka.clients.consumer.ConsumerConfig -293  [main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = latest
	bootstrap.servers = [kafka-1:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = DemoConsumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[framework] 2020-06-21 08:48:01,960 - org.apache.kafka.clients.consumer.KafkaConsumer -293  [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - [Consumer clientId=consumer, groupId=DemoConsumer] Initializing the Kafka consumer
[framework] 2020-06-21 08:48:01,961 - org.apache.kafka.common.metrics.Metrics -294  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-throttle-time
[framework] 2020-06-21 08:48:01,962 - org.apache.kafka.common.metrics.Metrics -295  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-closed:
[framework] 2020-06-21 08:48:01,962 - org.apache.kafka.common.metrics.Metrics -295  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-created:
[framework] 2020-06-21 08:48:01,962 - org.apache.kafka.common.metrics.Metrics -295  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name successful-authentication:
[framework] 2020-06-21 08:48:01,963 - org.apache.kafka.common.metrics.Metrics -296  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name successful-reauthentication:
[framework] 2020-06-21 08:48:01,963 - org.apache.kafka.common.metrics.Metrics -296  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name successful-authentication-no-reauth:
[framework] 2020-06-21 08:48:01,963 - org.apache.kafka.common.metrics.Metrics -296  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name failed-authentication:
[framework] 2020-06-21 08:48:01,964 - org.apache.kafka.common.metrics.Metrics -297  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name failed-reauthentication:
[framework] 2020-06-21 08:48:01,964 - org.apache.kafka.common.metrics.Metrics -297  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name reauthentication-latency:
[framework] 2020-06-21 08:48:01,964 - org.apache.kafka.common.metrics.Metrics -297  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent-received:
[framework] 2020-06-21 08:48:01,965 - org.apache.kafka.common.metrics.Metrics -298  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent:
[framework] 2020-06-21 08:48:01,966 - org.apache.kafka.common.metrics.Metrics -299  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-received:
[framework] 2020-06-21 08:48:01,966 - org.apache.kafka.common.metrics.Metrics -299  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name select-time:
[framework] 2020-06-21 08:48:01,967 - org.apache.kafka.common.metrics.Metrics -300  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name io-time:
[framework] 2020-06-21 08:48:01,968 - org.apache.kafka.common.metrics.Metrics -301  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name heartbeat-latency
[framework] 2020-06-21 08:48:01,968 - org.apache.kafka.common.metrics.Metrics -301  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name join-latency
[framework] 2020-06-21 08:48:01,969 - org.apache.kafka.common.metrics.Metrics -302  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name sync-latency
[framework] 2020-06-21 08:48:01,969 - org.apache.kafka.common.metrics.Metrics -302  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name commit-latency
[framework] 2020-06-21 08:48:01,970 - org.apache.kafka.common.metrics.Metrics -303  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-fetched
[framework] 2020-06-21 08:48:01,971 - org.apache.kafka.common.metrics.Metrics -304  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-fetched
[framework] 2020-06-21 08:48:01,971 - org.apache.kafka.common.metrics.Metrics -304  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-latency
[framework] 2020-06-21 08:48:01,972 - org.apache.kafka.common.metrics.Metrics -305  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-lag
[framework] 2020-06-21 08:48:01,972 - org.apache.kafka.common.metrics.Metrics -305  [main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-lead
[framework] 2020-06-21 08:48:01,972 - org.apache.kafka.common.utils.AppInfoParser -305  [main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version: 2.3.0
[framework] 2020-06-21 08:48:01,972 - org.apache.kafka.common.utils.AppInfoParser -305  [main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId: fc1aaa116b661c8a
[framework] 2020-06-21 08:48:01,972 - org.apache.kafka.common.utils.AppInfoParser -305  [main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka startTimeMs: 1592700481972
[framework] 2020-06-21 08:48:01,973 - org.apache.kafka.common.utils.AppInfoParser -306  [main] WARN  org.apache.kafka.common.utils.AppInfoParser  - Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.consumer:type=app-info,id=consumer
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:821)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:664)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:644)
	at cn.neyzoter.module.kafka.KafkaComsumerCli.<init>(KafkaComsumerCli.java:35)
	at cn.neyzoter.module.kafka.KafkaApp.main(KafkaApp.java:16)
[framework] 2020-06-21 08:48:01,976 - org.apache.kafka.clients.consumer.KafkaConsumer -309  [main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - [Consumer clientId=consumer, groupId=DemoConsumer] Kafka consumer initialized
[framework] 2020-06-21 08:48:01,979 - cn.neyzoter.module.kafka.KafkaComsumerCli -312  [KafkaConsumerExample] INFO  cn.neyzoter.module.kafka.KafkaComsumerCli  - [KafkaConsumerExample]: Starting
[framework] 2020-06-21 08:48:01,979 - cn.neyzoter.module.kafka.KafkaComsumerCli -312  [KafkaConsumerExample] INFO  cn.neyzoter.module.kafka.KafkaComsumerCli  - [KafkaConsumerExample]: Starting
[framework] 2020-06-21 08:48:01,980 - org.apache.kafka.clients.consumer.KafkaConsumer -313  [KafkaConsumerExample] INFO  org.apache.kafka.clients.consumer.KafkaConsumer  - [Consumer clientId=consumer, groupId=DemoConsumer] Subscribed to topic(s): topic1
[framework] 2020-06-21 08:48:01,980 - org.apache.kafka.clients.consumer.KafkaConsumer -313  [KafkaConsumerExample] INFO  org.apache.kafka.clients.consumer.KafkaConsumer  - [Consumer clientId=consumer, groupId=DemoConsumer] Subscribed to topic(s): topic1
[framework] 2020-06-21 08:48:01,982 - cn.neyzoter.module.kafka.KafkaComsumerCli -315  [KafkaConsumerExample] ERROR cn.neyzoter.module.kafka.KafkaComsumerCli  - [KafkaConsumerExample]: Error due to
java.util.MissingFormatArgumentException: Format specifier '%d'
	at java.util.Formatter.format(Formatter.java:2519)
	at java.util.Formatter.format(Formatter.java:2455)
	at java.lang.String.format(String.java:2940)
	at cn.neyzoter.module.kafka.KafkaComsumerCli.doWork(KafkaComsumerCli.java:43)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:89)
[framework] 2020-06-21 08:48:01,983 - cn.neyzoter.module.kafka.KafkaComsumerCli -316  [KafkaConsumerExample] INFO  cn.neyzoter.module.kafka.KafkaComsumerCli  - [KafkaConsumerExample]: Stopped
[framework] 2020-06-21 08:48:01,985 - cn.neyzoter.module.kafka.KafkaComsumerCli -318  [KafkaConsumerExample] ERROR cn.neyzoter.module.kafka.KafkaComsumerCli  - [KafkaConsumerExample]: Error due to
java.util.MissingFormatArgumentException: Format specifier '%d'
	at java.util.Formatter.format(Formatter.java:2519)
	at java.util.Formatter.format(Formatter.java:2455)
	at java.lang.String.format(String.java:2940)
	at cn.neyzoter.module.kafka.KafkaComsumerCli.doWork(KafkaComsumerCli.java:43)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:89)
[framework] 2020-06-21 08:48:01,985 - cn.neyzoter.module.kafka.KafkaComsumerCli -318  [KafkaConsumerExample] INFO  cn.neyzoter.module.kafka.KafkaComsumerCli  - [KafkaConsumerExample]: Stopped
[framework] 2020-06-21 08:48:02,063 - org.apache.kafka.clients.NetworkClient -396  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Completed connection to node -1. Fetching API versions.
[framework] 2020-06-21 08:48:02,063 - org.apache.kafka.clients.NetworkClient -396  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Initiating API versions fetch from node -1.
[framework] 2020-06-21 08:48:02,077 - org.apache.kafka.clients.NetworkClient -410  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Recorded API versions for node -1: (Produce(0): 0 to 7 [usable: 7], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 8 [usable: 8], LeaderAndIsr(4): 0 to 2 [usable: 2], StopReplica(5): 0 to 1 [usable: 1], UpdateMetadata(6): 0 to 5 [usable: 5], ControlledShutdown(7): 0 to 2 [usable: 2], OffsetCommit(8): 0 to 7 [usable: 7], OffsetFetch(9): 0 to 5 [usable: 5], FindCoordinator(10): 0 to 2 [usable: 2], JoinGroup(11): 0 to 5 [usable: 5], Heartbeat(12): 0 to 3 [usable: 3], LeaveGroup(13): 0 to 2 [usable: 2], SyncGroup(14): 0 to 3 [usable: 3], DescribeGroups(15): 0 to 3 [usable: 3], ListGroups(16): 0 to 2 [usable: 2], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 2 [usable: 2], CreateTopics(19): 0 to 3 [usable: 3], DeleteTopics(20): 0 to 3 [usable: 3], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 1 [usable: 1], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 1 [usable: 1], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 1 [usable: 1], ElectPreferredLeaders(43): 0 [usable: 0], IncrementalAlterConfigs(44): 0 [usable: 0])
[framework] 2020-06-21 08:48:02,079 - org.apache.kafka.clients.NetworkClient -412  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Sending metadata request MetadataRequestData(topics=[], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node kafka-1:9092 (id: -1 rack: null)
[framework] 2020-06-21 08:48:02,096 - org.apache.kafka.clients.Metadata -429  [kafka-producer-network-thread | DemoProducer] INFO  org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Cluster ID: MT1saCQCQj6WuKonjf5obA
[framework] 2020-06-21 08:48:02,096 - org.apache.kafka.clients.Metadata -429  [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updated cluster metadata updateVersion 2 to MetadataCache{cluster=Cluster(id = MT1saCQCQj6WuKonjf5obA, nodes = [Kafka-1:9092 (id: 1 rack: null), Kafka-1:9093 (id: 2 rack: null)], partitions = [], controller = Kafka-1:9092 (id: 1 rack: null))}
[framework] 2020-06-21 08:48:02,860 - org.apache.kafka.clients.NetworkClient -1193 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Initialize connection to node Kafka-1:9093 (id: 2 rack: null) for sending metadata request
[framework] 2020-06-21 08:48:02,861 - org.apache.kafka.clients.NetworkClient -1194 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Initiating connection to node Kafka-1:9093 (id: 2 rack: null) using address kafka-1/47.110.251.155
[framework] 2020-06-21 08:48:02,866 - org.apache.kafka.common.metrics.Metrics -1199 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2.bytes-sent
[framework] 2020-06-21 08:48:02,867 - org.apache.kafka.common.metrics.Metrics -1200 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2.bytes-received
[framework] 2020-06-21 08:48:02,868 - org.apache.kafka.common.metrics.Metrics -1201 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2.latency
[framework] 2020-06-21 08:48:02,869 - org.apache.kafka.common.network.Selector -1202 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.network.Selector  - [Producer clientId=DemoProducer] Created socket with SO_RCVBUF = 32768, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node 2
[framework] 2020-06-21 08:48:02,869 - org.apache.kafka.clients.NetworkClient -1202 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Completed connection to node 2. Fetching API versions.
[framework] 2020-06-21 08:48:02,869 - org.apache.kafka.clients.NetworkClient -1202 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Initiating API versions fetch from node 2.
[framework] 2020-06-21 08:48:02,873 - org.apache.kafka.clients.NetworkClient -1206 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Recorded API versions for node 2: (Produce(0): 0 to 7 [usable: 7], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 8 [usable: 8], LeaderAndIsr(4): 0 to 2 [usable: 2], StopReplica(5): 0 to 1 [usable: 1], UpdateMetadata(6): 0 to 5 [usable: 5], ControlledShutdown(7): 0 to 2 [usable: 2], OffsetCommit(8): 0 to 7 [usable: 7], OffsetFetch(9): 0 to 5 [usable: 5], FindCoordinator(10): 0 to 2 [usable: 2], JoinGroup(11): 0 to 5 [usable: 5], Heartbeat(12): 0 to 3 [usable: 3], LeaveGroup(13): 0 to 2 [usable: 2], SyncGroup(14): 0 to 3 [usable: 3], DescribeGroups(15): 0 to 3 [usable: 3], ListGroups(16): 0 to 2 [usable: 2], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 2 [usable: 2], CreateTopics(19): 0 to 3 [usable: 3], DeleteTopics(20): 0 to 3 [usable: 3], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 1 [usable: 1], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 1 [usable: 1], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 1 [usable: 1], ElectPreferredLeaders(43): 0 [usable: 0], IncrementalAlterConfigs(44): 0 [usable: 0])
[framework] 2020-06-21 08:48:02,873 - org.apache.kafka.clients.NetworkClient -1206 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='topic1')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node Kafka-1:9093 (id: 2 rack: null)
[framework] 2020-06-21 08:48:02,911 - org.apache.kafka.clients.Metadata -1244 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from null to 0 for partition topic1-0
[framework] 2020-06-21 08:48:02,912 - org.apache.kafka.clients.Metadata -1245 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from null to 0 for partition topic1-2
[framework] 2020-06-21 08:48:02,912 - org.apache.kafka.clients.Metadata -1245 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from null to 0 for partition topic1-1
[framework] 2020-06-21 08:48:02,916 - org.apache.kafka.clients.Metadata -1249 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updated cluster metadata updateVersion 3 to MetadataCache{cluster=Cluster(id = MT1saCQCQj6WuKonjf5obA, nodes = [Kafka-1:9093 (id: 2 rack: null), Kafka-1:9092 (id: 1 rack: null)], partitions = [Partition(topic = topic1, partition = 1, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), Partition(topic = topic1, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), Partition(topic = topic1, partition = 2, leader = 1, replicas = [1], isr = [1], offlineReplicas = [])], controller = Kafka-1:9092 (id: 1 rack: null))}
[framework] 2020-06-21 08:48:02,936 - org.apache.kafka.clients.NetworkClient -1269 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Initiating connection to node Kafka-1:9092 (id: 1 rack: null) using address kafka-1/47.110.251.155
[framework] 2020-06-21 08:48:02,939 - org.apache.kafka.common.metrics.Metrics -1272 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-1.bytes-sent
[framework] 2020-06-21 08:48:02,940 - org.apache.kafka.common.metrics.Metrics -1273 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-1.bytes-received
[framework] 2020-06-21 08:48:02,940 - org.apache.kafka.common.metrics.Metrics -1273 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-1.latency
[framework] 2020-06-21 08:48:02,941 - org.apache.kafka.common.network.Selector -1274 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.network.Selector  - [Producer clientId=DemoProducer] Created socket with SO_RCVBUF = 32768, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node 1
[framework] 2020-06-21 08:48:02,941 - org.apache.kafka.clients.NetworkClient -1274 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Completed connection to node 1. Fetching API versions.
[framework] 2020-06-21 08:48:02,941 - org.apache.kafka.clients.NetworkClient -1274 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Initiating API versions fetch from node 1.
[framework] 2020-06-21 08:48:02,961 - org.apache.kafka.clients.NetworkClient -1294 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Recorded API versions for node 1: (Produce(0): 0 to 7 [usable: 7], Fetch(1): 0 to 11 [usable: 11], ListOffsets(2): 0 to 5 [usable: 5], Metadata(3): 0 to 8 [usable: 8], LeaderAndIsr(4): 0 to 2 [usable: 2], StopReplica(5): 0 to 1 [usable: 1], UpdateMetadata(6): 0 to 5 [usable: 5], ControlledShutdown(7): 0 to 2 [usable: 2], OffsetCommit(8): 0 to 7 [usable: 7], OffsetFetch(9): 0 to 5 [usable: 5], FindCoordinator(10): 0 to 2 [usable: 2], JoinGroup(11): 0 to 5 [usable: 5], Heartbeat(12): 0 to 3 [usable: 3], LeaveGroup(13): 0 to 2 [usable: 2], SyncGroup(14): 0 to 3 [usable: 3], DescribeGroups(15): 0 to 3 [usable: 3], ListGroups(16): 0 to 2 [usable: 2], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 2 [usable: 2], CreateTopics(19): 0 to 3 [usable: 3], DeleteTopics(20): 0 to 3 [usable: 3], DeleteRecords(21): 0 to 1 [usable: 1], InitProducerId(22): 0 to 1 [usable: 1], OffsetForLeaderEpoch(23): 0 to 3 [usable: 3], AddPartitionsToTxn(24): 0 to 1 [usable: 1], AddOffsetsToTxn(25): 0 to 1 [usable: 1], EndTxn(26): 0 to 1 [usable: 1], WriteTxnMarkers(27): 0 [usable: 0], TxnOffsetCommit(28): 0 to 2 [usable: 2], DescribeAcls(29): 0 to 1 [usable: 1], CreateAcls(30): 0 to 1 [usable: 1], DeleteAcls(31): 0 to 1 [usable: 1], DescribeConfigs(32): 0 to 2 [usable: 2], AlterConfigs(33): 0 to 1 [usable: 1], AlterReplicaLogDirs(34): 0 to 1 [usable: 1], DescribeLogDirs(35): 0 to 1 [usable: 1], SaslAuthenticate(36): 0 to 1 [usable: 1], CreatePartitions(37): 0 to 1 [usable: 1], CreateDelegationToken(38): 0 to 1 [usable: 1], RenewDelegationToken(39): 0 to 1 [usable: 1], ExpireDelegationToken(40): 0 to 1 [usable: 1], DescribeDelegationToken(41): 0 to 1 [usable: 1], DeleteGroups(42): 0 to 1 [usable: 1], ElectPreferredLeaders(43): 0 [usable: 0], IncrementalAlterConfigs(44): 0 [usable: 0])
[framework] 2020-06-21 08:48:02,965 - org.apache.kafka.common.metrics.Metrics -1298 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.topic1.records-per-batch
[framework] 2020-06-21 08:48:02,966 - org.apache.kafka.common.metrics.Metrics -1299 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.topic1.bytes
[framework] 2020-06-21 08:48:02,966 - org.apache.kafka.common.metrics.Metrics -1299 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.topic1.compression-rate
[framework] 2020-06-21 08:48:02,967 - org.apache.kafka.common.metrics.Metrics -1300 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.topic1.record-retries
[framework] 2020-06-21 08:48:02,967 - org.apache.kafka.common.metrics.Metrics -1300 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.topic1.record-errors
[framework] 2020-06-21 08:53:02,880 - org.apache.kafka.clients.NetworkClient -301213 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='topic1')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node Kafka-1:9092 (id: 1 rack: null)
[framework] 2020-06-21 08:53:02,887 - org.apache.kafka.clients.Metadata -301220 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from 0 to 0 for partition topic1-0
[framework] 2020-06-21 08:53:02,887 - org.apache.kafka.clients.Metadata -301220 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from 0 to 0 for partition topic1-2
[framework] 2020-06-21 08:53:02,888 - org.apache.kafka.clients.Metadata -301221 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from 0 to 0 for partition topic1-1
[framework] 2020-06-21 08:53:02,889 - org.apache.kafka.clients.Metadata -301222 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updated cluster metadata updateVersion 4 to MetadataCache{cluster=Cluster(id = MT1saCQCQj6WuKonjf5obA, nodes = [Kafka-1:9092 (id: 1 rack: null), Kafka-1:9093 (id: 2 rack: null)], partitions = [Partition(topic = topic1, partition = 1, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), Partition(topic = topic1, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), Partition(topic = topic1, partition = 2, leader = 1, replicas = [1], isr = [1], offlineReplicas = [])], controller = Kafka-1:9092 (id: 1 rack: null))}
[framework] 2020-06-21 08:57:02,302 - org.apache.kafka.clients.NetworkClient -540635 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Node -1 disconnected.
[framework] 2020-06-21 08:57:02,307 - org.apache.kafka.clients.NetworkClient -540640 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='topic1')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node Kafka-1:9093 (id: 2 rack: null)
[framework] 2020-06-21 08:57:02,315 - org.apache.kafka.clients.Metadata -540648 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from 0 to 0 for partition topic1-0
[framework] 2020-06-21 08:57:02,315 - org.apache.kafka.clients.Metadata -540648 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from 0 to 0 for partition topic1-2
[framework] 2020-06-21 08:57:02,315 - org.apache.kafka.clients.Metadata -540648 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from 0 to 0 for partition topic1-1
[framework] 2020-06-21 08:57:02,317 - org.apache.kafka.clients.Metadata -540650 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updated cluster metadata updateVersion 5 to MetadataCache{cluster=Cluster(id = MT1saCQCQj6WuKonjf5obA, nodes = [Kafka-1:9093 (id: 2 rack: null), Kafka-1:9092 (id: 1 rack: null)], partitions = [Partition(topic = topic1, partition = 1, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), Partition(topic = topic1, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), Partition(topic = topic1, partition = 2, leader = 1, replicas = [1], isr = [1], offlineReplicas = [])], controller = Kafka-1:9092 (id: 1 rack: null))}
[framework] 2020-06-21 09:02:02,315 - org.apache.kafka.clients.NetworkClient -840648 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='topic1')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node Kafka-1:9093 (id: 2 rack: null)
[framework] 2020-06-21 09:02:02,323 - org.apache.kafka.clients.Metadata -840656 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from 0 to 0 for partition topic1-0
[framework] 2020-06-21 09:02:02,323 - org.apache.kafka.clients.Metadata -840656 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from 0 to 0 for partition topic1-2
[framework] 2020-06-21 09:02:02,324 - org.apache.kafka.clients.Metadata -840657 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from 0 to 0 for partition topic1-1
[framework] 2020-06-21 09:02:02,326 - org.apache.kafka.clients.Metadata -840659 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updated cluster metadata updateVersion 6 to MetadataCache{cluster=Cluster(id = MT1saCQCQj6WuKonjf5obA, nodes = [Kafka-1:9093 (id: 2 rack: null), Kafka-1:9092 (id: 1 rack: null)], partitions = [Partition(topic = topic1, partition = 1, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), Partition(topic = topic1, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), Partition(topic = topic1, partition = 2, leader = 1, replicas = [1], isr = [1], offlineReplicas = [])], controller = Kafka-1:9092 (id: 1 rack: null))}
[framework] 2020-06-21 09:07:02,322 - org.apache.kafka.clients.NetworkClient -1140655 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='topic1')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node Kafka-1:9093 (id: 2 rack: null)
[framework] 2020-06-21 09:07:02,329 - org.apache.kafka.clients.Metadata -1140662 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from 0 to 0 for partition topic1-0
[framework] 2020-06-21 09:07:02,329 - org.apache.kafka.clients.Metadata -1140662 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from 0 to 0 for partition topic1-2
[framework] 2020-06-21 09:07:02,329 - org.apache.kafka.clients.Metadata -1140662 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from 0 to 0 for partition topic1-1
[framework] 2020-06-21 09:07:02,331 - org.apache.kafka.clients.Metadata -1140664 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updated cluster metadata updateVersion 7 to MetadataCache{cluster=Cluster(id = MT1saCQCQj6WuKonjf5obA, nodes = [Kafka-1:9092 (id: 1 rack: null), Kafka-1:9093 (id: 2 rack: null)], partitions = [Partition(topic = topic1, partition = 1, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), Partition(topic = topic1, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), Partition(topic = topic1, partition = 2, leader = 1, replicas = [1], isr = [1], offlineReplicas = [])], controller = Kafka-1:9092 (id: 1 rack: null))}
[framework] 2020-06-21 09:12:02,328 - org.apache.kafka.clients.NetworkClient -1440661 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.NetworkClient  - [Producer clientId=DemoProducer] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(name='topic1')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node Kafka-1:9093 (id: 2 rack: null)
[framework] 2020-06-21 09:12:02,335 - org.apache.kafka.clients.Metadata -1440668 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from 0 to 0 for partition topic1-0
[framework] 2020-06-21 09:12:02,335 - org.apache.kafka.clients.Metadata -1440668 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from 0 to 0 for partition topic1-2
[framework] 2020-06-21 09:12:02,335 - org.apache.kafka.clients.Metadata -1440668 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updating last seen epoch from 0 to 0 for partition topic1-1
[framework] 2020-06-21 09:12:02,337 - org.apache.kafka.clients.Metadata -1440670 [kafka-producer-network-thread | DemoProducer] DEBUG org.apache.kafka.clients.Metadata  - [Producer clientId=DemoProducer] Updated cluster metadata updateVersion 8 to MetadataCache{cluster=Cluster(id = MT1saCQCQj6WuKonjf5obA, nodes = [Kafka-1:9093 (id: 2 rack: null), Kafka-1:9092 (id: 1 rack: null)], partitions = [Partition(topic = topic1, partition = 1, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), Partition(topic = topic1, partition = 0, leader = 1, replicas = [1], isr = [1], offlineReplicas = []), Partition(topic = topic1, partition = 2, leader = 1, replicas = [1], isr = [1], offlineReplicas = [])], controller = Kafka-1:9092 (id: 1 rack: null))}
